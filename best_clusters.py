# -*- coding: utf-8 -*-
"""best_clusters.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bpVvplLk7eRBgZMXE2bJ6gOYYm1FoX3Z
"""

import pandas as pd
import re
from collections import defaultdict
from tqdm import tqdm

# Read the CSV file
df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)

# Function to remove specified hashtags from a text
def remove_hashtags(text):
    hashtags_to_remove = ['#blm', '#black_lives_matter', '#blacklivesmatter']
    for hashtag in hashtags_to_remove:
        text = text.replace(hashtag, '')
    return text

# Remove specified hashtags from the content column
df['content'] = df['content'].apply(lambda x: remove_hashtags(x))

# Function to extract hashtags from text
def extract_hashtags(text):
    return re.findall(r'#\w+', text)

# Build co-occurrence graph
co_occurrence_graph = defaultdict(lambda: defaultdict(int))

for content in df['content']:
    hashtags = extract_hashtags(content)
    for i in range(len(hashtags)):
        for j in range(i+1, len(hashtags)):
            co_occurrence_graph[hashtags[i]][hashtags[j]] += 1
            co_occurrence_graph[hashtags[j]][hashtags[i]] += 1

# Function for DFS to find connected components
def dfs_iterative(start_node):
    visited = set()
    stack = [start_node]
    cluster = []

    while stack:
        node = stack.pop()
        if node not in visited:
            visited.add(node)
            cluster.append(node)
            stack.extend(co_occurrence_graph[node].keys() - visited)

    return cluster

# Find clusters using DFS with tqdm progress bar
visited = set()
clusters = []

# tqdm for progress bar
for hashtag in tqdm(co_occurrence_graph):
    if hashtag not in visited:
        cluster = dfs_iterative(hashtag)
        clusters.append(cluster)

# Print clusters
for i, cluster in enumerate(clusters):
    print(f'Cluster {i+1}: {cluster}')

print("Number of unique hashtags:", len(co_occurrence_graph))

import pandas as pd
import re
from collections import defaultdict, Counter
from tqdm import tqdm

# Read the CSV file
df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)

# Function to remove specified hashtags from a text
def remove_hashtags(text):
    hashtags_to_remove = ['#blm', '#black_lives_matter', '#blacklivesmatter']
    for hashtag in hashtags_to_remove:
        text = text.replace(hashtag, '')
    return text

# Remove specified hashtags from the content column
df['content'] = df['content'].apply(lambda x: remove_hashtags(x))

# Function to extract hashtags from text
def extract_hashtags(text):
    return re.findall(r'#\w+', text)

# Count hashtag occurrences
hashtags_counter = Counter()
for content in df['content']:
    hashtags = extract_hashtags(content)
    hashtags_counter.update(hashtags)

# Select top 1000 hashtags based on frequency
top_hashtags = [hashtag for hashtag, _ in hashtags_counter.most_common(1000)]

# Build co-occurrence graph using only top 1000 hashtags
co_occurrence_graph = defaultdict(lambda: defaultdict(int))

for content in tqdm(df['content']):
    hashtags = extract_hashtags(content)
    hashtags = [hashtag for hashtag in hashtags if hashtag in top_hashtags]
    for i in range(len(hashtags)):
        for j in range(i+1, len(hashtags)):
            co_occurrence_graph[hashtags[i]][hashtags[j]] += 1
            co_occurrence_graph[hashtags[j]][hashtags[i]] += 1

# Function for DFS to find connected components
def dfs_iterative(start_node):
    visited = set()
    stack = [start_node]
    cluster = []

    while stack:
        node = stack.pop()
        if node not in visited:
            visited.add(node)
            cluster.append(node)
            stack.extend(co_occurrence_graph[node].keys() - visited)

    return cluster

# Find clusters using DFS with tqdm progress bar
visited = set()
clusters = []

# tqdm for progress bar
for hashtag in tqdm(co_occurrence_graph):
    if hashtag not in visited:
        cluster = dfs_iterative(hashtag)
        clusters.append(cluster)

# Print clusters
for i, cluster in enumerate(clusters):
    print(f'Cluster {i+1}: {cluster}')

# Print adjacency lists
for hashtag, neighbors in co_occurrence_graph.items():
    print(f'{hashtag}: {list(neighbors.keys())}')

import pandas as pd
import re
from collections import defaultdict, Counter
from tqdm import tqdm

# Read the CSV file
df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)

# Function to remove specified hashtags from a text
def remove_hashtags(text):
    hashtags_to_remove = ['#blm', '#black_lives_matter', '#blacklivesmatter']
    for hashtag in hashtags_to_remove:
        text = text.replace(hashtag, '')
    return text

# Remove specified hashtags from the content column
df['content'] = df['content'].apply(lambda x: remove_hashtags(x))

# Function to extract hashtags from text
def extract_hashtags(text):
    return re.findall(r'#\w+', text)

# Count hashtag occurrences
hashtags_counter = Counter()
for content in df['content']:
    hashtags = extract_hashtags(content)
    hashtags_counter.update(hashtags)

# Select top 1000 hashtags based on frequency
top_hashtags = [hashtag for hashtag, _ in hashtags_counter.most_common(1000)]

# Build co-occurrence graph using only top 1000 hashtags
co_occurrence_graph = defaultdict(lambda: defaultdict(int))

for content in tqdm(df['content']):
    hashtags = extract_hashtags(content)
    hashtags = [hashtag for hashtag in hashtags if hashtag in top_hashtags]
    for i in range(len(hashtags)):
        for j in range(i+1, len(hashtags)):
            co_occurrence_graph[hashtags[i]][hashtags[j]] += 1
            co_occurrence_graph[hashtags[j]][hashtags[i]] += 1

# Function for iterative DFS to find connected components
def dfs_iterative(start_node, co_occurrence_graph):
    visited = set()
    stack = [start_node]
    cluster = []

    while stack:
        node = stack.pop()
        if node not in visited:
            visited.add(node)
            cluster.append(node)
            for neighbor in co_occurrence_graph[node]:
                if neighbor not in visited:
                    stack.append(neighbor)

    return cluster

# Find clusters using iterative DFS with tqdm progress bar
visited = set()
clusters = []

# tqdm for progress bar
for hashtag in tqdm(co_occurrence_graph):
    if hashtag not in visited:
        cluster = dfs_iterative(hashtag, co_occurrence_graph)
        clusters.append(cluster)
        visited.update(cluster)

# Print clusters
for i, cluster in enumerate(clusters):
    print(f'Cluster {i+1}: {cluster}')

import pandas as pd
import re
from collections import defaultdict, Counter

# Read the CSV file
df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)

# Function to remove specified hashtags from a text
def remove_hashtags(text, hashtags_to_remove):
    for hashtag in hashtags_to_remove:
        text = text.replace(hashtag, '')
    return text

# Remove specified hashtags from the content column
hashtags_to_remove = ['#blm','#BLACK_LIVES_MATTER', '#black_lives_matter','#BlackLivesMatter' , '#AllBlackLivesMatter','#blacklivesmatter', '#BLACKlivesmatter', '#black_lifes_matter', '#BLM', '#blmProtest']
df['content'] = df['content'].apply(lambda x: remove_hashtags(x, hashtags_to_remove))

# Function to extract hashtags from text
def extract_hashtags(text):
    return re.findall(r'#\w+', text)

# Count hashtag occurrences
hashtags_counter = Counter()
for content in df['content']:
    hashtags = extract_hashtags(content)
    hashtags_counter.update(hashtags)

# Get top 10 most frequent hashtags after removing specified hashtags
top_10_hashtags = [hashtag for hashtag, _ in hashtags_counter.most_common(10)]

# Build co-occurrence graph using only top 10 hashtags
co_occurrence_graph = defaultdict(lambda: defaultdict(int))

for content in df['content']:
    hashtags = extract_hashtags(content)
    hashtags = [hashtag for hashtag in hashtags if hashtag in top_10_hashtags]
    for i in range(len(hashtags)):
        for j in range(i+1, len(hashtags)):
            co_occurrence_graph[hashtags[i]][hashtags[j]] += 1
            co_occurrence_graph[hashtags[j]][hashtags[i]] += 1

# Print adjacency lists for top 10 hashtags
for hashtag in top_10_hashtags:
    neighbors = co_occurrence_graph[hashtag]
    print(f'Adjacency list for #{hashtag}: {list(neighbors.keys())}')

import pandas as pd
import re
import matplotlib.pyplot as plt
import networkx as nx
from collections import defaultdict, Counter

# Read the CSV file
df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)

# Function to remove specified hashtags from a text
def remove_hashtags(text, hashtags_to_remove):
    for hashtag in hashtags_to_remove:
        text = text.replace(hashtag, '')
    return text

# Remove specified hashtags from the content column
hashtags_to_remove = ['#blm','#BLACK_LIVES_MATTER', '#black_lives_matter','#BlackLivesMatter' , '#AllBlackLivesMatter','#blacklivesmatter', '#BLACKlivesmatter', '#black_lifes_matter', '#BLM', '#blmProtest']
df['content'] = df['content'].apply(lambda x: remove_hashtags(x, hashtags_to_remove))

# Function to extract hashtags from text
def extract_hashtags(text):
    return re.findall(r'#\w+', text)

# Count hashtag occurrences
hashtags_counter = Counter()
for content in df['content']:
    hashtags = extract_hashtags(content)
    hashtags_counter.update(hashtags)

# Get top 10 most frequent hashtags after removing specified hashtags
top_10_hashtags = [hashtag for hashtag, _ in hashtags_counter.most_common(10)]

# Build co-occurrence graph using only top 10 hashtags
co_occurrence_graph = defaultdict(lambda: defaultdict(int))

for content in df['content']:
    hashtags = extract_hashtags(content)
    hashtags = [hashtag for hashtag in hashtags if hashtag in top_10_hashtags]
    for i in range(len(hashtags)):
        for j in range(i+1, len(hashtags)):
            co_occurrence_graph[hashtags[i]][hashtags[j]] += 1
            co_occurrence_graph[hashtags[j]][hashtags[i]] += 1

# Create a directed graph
G = nx.Graph()

# Add nodes and edges with weights
for hashtag, neighbors in co_occurrence_graph.items():
    for neighbor, weight in neighbors.items():
        G.add_edge(hashtag, neighbor, weight=weight)

# Plot the graph
pos = nx.spring_layout(G, seed=42)  # Set the layout for consistent positioning
nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=2000, font_size=10, font_weight='bold')
edge_labels = nx.get_edge_attributes(G, 'weight')
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red')
plt.title('Co-occurrence Graph of Top 10 Hashtags')
plt.show()

import pandas as pd
import re
import matplotlib.pyplot as plt
import networkx as nx
from collections import defaultdict
from tqdm import tqdm

# Read the CSV file
df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)

# Function to remove specified hashtags from a text
def remove_hashtags(text, hashtags_to_remove):
    for hashtag in hashtags_to_remove:
        text = text.replace(hashtag, '')
    return text

# Remove specified hashtags from the content column
hashtags_to_remove = ['#blm','#BLACK_LIVES_MATTER', '#black_lives_matter','#BlackLivesMatter' , '#AllBlackLivesMatter','#blacklivesmatter', '#BLACKlivesmatter', '#black_lifes_matter', '#BLM', '#blmProtest', '#BLACKLIVESMATTER']
df['content'] = df['content'].apply(lambda x: remove_hashtags(x, hashtags_to_remove))

# Function to extract hashtags from text
def extract_hashtags(text):
    return re.findall(r'#\w+', text)

# Count hashtag occurrences
hashtags_counter = Counter()
for content in df['content']:
    hashtags = extract_hashtags(content)
    hashtags_counter.update(hashtags)

# Get top 300 most frequent hashtags after removing specified hashtags
top_300_hashtags = [hashtag for hashtag, _ in hashtags_counter.most_common(200)]

# Build co-occurrence graph using only top 300 hashtags
co_occurrence_graph = defaultdict(lambda: defaultdict(int))

for content in tqdm(df['content']):
    hashtags = extract_hashtags(content)
    hashtags = [hashtag for hashtag in hashtags if hashtag in top_300_hashtags]
    for i in range(len(hashtags)):
        for j in range(i+1, len(hashtags)):
            co_occurrence_graph[hashtags[i]][hashtags[j]] += 1
            co_occurrence_graph[hashtags[j]][hashtags[i]] += 1

# Remove edges with weights less than 10
co_occurrence_graph_filtered = defaultdict(lambda: defaultdict(int))

for hashtag, neighbors in co_occurrence_graph.items():
    for neighbor, weight in neighbors.items():
        if weight >= 10:
            co_occurrence_graph_filtered[hashtag][neighbor] = weight

# Create a directed graph
G = nx.Graph()

# Add nodes and edges with weights
for hashtag, neighbors in co_occurrence_graph_filtered.items():
    if neighbors:  # Add only if the node has neighbors
        for neighbor, weight in neighbors.items():
            G.add_edge(hashtag, neighbor, weight=weight)

# Plot the graph
plt.figure(figsize=(12, 8))
pos = nx.spring_layout(G, seed=42)  # Set the layout for consistent positioning
nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=1000, font_size=10, font_weight='bold')
edge_labels = nx.get_edge_attributes(G, 'weight')
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red')
plt.title('Co-occurrence Graph after Removing Edges with Weight < 10 and Nodes with No Edges')
plt.show()

# Print number of nodes after removing nodes with no edges
print("Number of nodes after removing nodes with no edges:", G.number_of_nodes())

import pandas as pd
import re
from collections import defaultdict
from tqdm import tqdm

# Read the CSV file
df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)

# Function to remove specified hashtags from a text
def remove_hashtags(text, hashtags_to_remove):
    for hashtag in hashtags_to_remove:
        text = text.replace(hashtag, '')
    return text

# Remove specified hashtags from the content column
hashtags_to_remove = ['#blm','#BLACK_LIVES_MATTER', '#black_lives_matter','#BlackLivesMatter' , '#AllBlackLivesMatter','#blacklivesmatter', '#BLACKlivesmatter', '#black_lifes_matter', '#BLM', '#blmProtest']
df['content'] = df['content'].apply(lambda x: remove_hashtags(x, hashtags_to_remove))

# Function to extract hashtags from text
def extract_hashtags(text):
    return re.findall(r'#\w+', text)

# Count hashtag occurrences
hashtags_counter = Counter()
for content in df['content']:
    hashtags = extract_hashtags(content)
    hashtags_counter.update(hashtags)

# Get top 300 most frequent hashtags after removing specified hashtags
top_300_hashtags = [hashtag for hashtag, _ in hashtags_counter.most_common(300)]

# Build co-occurrence graph using only top 300 hashtags
co_occurrence_graph = defaultdict(lambda: defaultdict(int))

for content in tqdm(df['content']):
    hashtags = extract_hashtags(content)
    hashtags = [hashtag for hashtag in hashtags if hashtag in top_300_hashtags]
    for i in range(len(hashtags)):
        for j in range(i+1, len(hashtags)):
            co_occurrence_graph[hashtags[i]][hashtags[j]] += 1
            co_occurrence_graph[hashtags[j]][hashtags[i]] += 1

# Remove edges with weights less than 10
co_occurrence_graph_filtered = defaultdict(lambda: defaultdict(int))

for hashtag, neighbors in co_occurrence_graph.items():
    for neighbor, weight in neighbors.items():
        if weight >= 10:
            co_occurrence_graph_filtered[hashtag][neighbor] = weight

# Function for DFS to find connected components
def dfs(node, visited, cluster):
    visited.add(node)
    cluster.append(node)
    for neighbor in co_occurrence_graph_filtered[node]:
        if neighbor not in visited:
            dfs(neighbor, visited, cluster)

# Find clusters using DFS
visited = set()
clusters = []

for hashtag in co_occurrence_graph_filtered:
    if hashtag not in visited:
        cluster = []
        dfs(hashtag, visited, cluster)
        clusters.append(cluster)

# Remove clusters with less than 5 entries
clusters = [cluster for cluster in clusters if len(cluster) >= 5]

# Print clusters
for i, cluster in enumerate(clusters):
    print(f'Cluster {i+1}: {cluster}')

import pandas as pd
import re
import plotly.graph_objects as go
from collections import defaultdict
from tqdm import tqdm

# Read the CSV file
df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)

# Function to remove specified hashtags from a text
def remove_hashtags(text, hashtags_to_remove):
    for hashtag in hashtags_to_remove:
        text = text.replace(hashtag, '')
    return text

# Remove specified hashtags from the content column
hashtags_to_remove = ['#blm','#BLACK_LIVES_MATTER', '#black_lives_matter','#BlackLivesMatter' , '#AllBlackLivesMatter','#blacklivesmatter', '#BLACKlivesmatter', '#black_lifes_matter', '#BLM', '#blmProtest']
df['content'] = df['content'].apply(lambda x: remove_hashtags(x, hashtags_to_remove))

# Function to extract hashtags from text
def extract_hashtags(text):
    return re.findall(r'#\w+', text)

# Count hashtag occurrences
hashtags_counter = Counter()
for content in df['content']:
    hashtags = extract_hashtags(content)
    hashtags_counter.update(hashtags)

# Get top 300 most frequent hashtags after removing specified hashtags
top_300_hashtags = [hashtag for hashtag, _ in hashtags_counter.most_common(300)]

# Build co-occurrence graph using only top 300 hashtags
co_occurrence_graph = defaultdict(lambda: defaultdict(int))

for content in tqdm(df['content']):
    hashtags = extract_hashtags(content)
    hashtags = [hashtag for hashtag in hashtags if hashtag in top_300_hashtags]
    for i in range(len(hashtags)):
        for j in range(i+1, len(hashtags)):
            co_occurrence_graph[hashtags[i]][hashtags[j]] += 1
            co_occurrence_graph[hashtags[j]][hashtags[i]] += 1

# Remove edges with weights less than 10
co_occurrence_graph_filtered = defaultdict(lambda: defaultdict(int))

for hashtag, neighbors in co_occurrence_graph.items():
    for neighbor, weight in neighbors.items():
        if weight >= 10:
            co_occurrence_graph_filtered[hashtag][neighbor] = weight

# Function for DFS to find connected components
def dfs(node, visited, cluster):
    visited.add(node)
    cluster.append(node)
    for neighbor in co_occurrence_graph_filtered[node]:
        if neighbor not in visited:
            dfs(neighbor, visited, cluster)

# Find clusters using DFS
visited = set()
clusters = []

for hashtag in co_occurrence_graph_filtered:
    if hashtag not in visited:
        cluster = []
        dfs(hashtag, visited, cluster)
        clusters.append(cluster)

# Remove clusters with less than 5 entries
clusters = [cluster for cluster in clusters if len(cluster) >= 5]

# Assign unique color to each cluster
cluster_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']

# Create Plotly graph
edge_trace = go.Scatter(
    x=[],
    y=[],
    line=dict(width=0.5, color='#888'),
    hoverinfo='none',
    mode='lines')

for hashtag, neighbors in co_occurrence_graph_filtered.items():
    for neighbor, weight in neighbors.items():
        x0, y0 = hashtag, neighbor
        edge_trace['x'] += (x0, y0, None)
        edge_trace['y'] += (x0, y0, None)

node_trace = go.Scatter(
    x=[],
    y=[],
    text=[],
    mode='markers',
    hoverinfo='text',
    marker=dict(
        showscale=True,
        colorscale='YlGnBu',
        reversescale=True,
        color=[],
        size=10,
        colorbar=dict(
            thickness=15,
            title='Node Connections',
            xanchor='left',
            titleside='right'
        ),
        line=dict(width=2)))

for cluster_id, cluster in enumerate(clusters):
    for node in cluster:
        x, y = node, node
        node_trace['x'] += (x,)
        node_trace['y'] += (y,)
        node_trace['text'] += (node,)
        node_trace['marker']['color'] += (cluster_colors[cluster_id],)

# Create Plotly figure
fig = go.Figure(data=[edge_trace, node_trace],
                layout=go.Layout(
                    title='Co-occurrence Graph with Clusters',
                    titlefont=dict(size=16),
                    showlegend=False,
                    hovermode='closest',
                    margin=dict(b=20, l=5, r=5, t=40),
                    annotations=[dict(
                        text="Python code: <a href='https://github.com/username/repo'> Github</a>",
                        showarrow=False,
                        xref="paper", yref="paper",
                        x=0.005, y=-0.002)],
                    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))

# Show the interactive Plotly graph
fig.show()

import pandas as pd
import re
import matplotlib.pyplot as plt
import networkx as nx
from collections import defaultdict
from tqdm import tqdm

# Read the CSV file
df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)

# Function to remove specified hashtags from a text
def remove_hashtags(text, hashtags_to_remove):
    for hashtag in hashtags_to_remove:
        text = text.replace(hashtag, '')
    return text

# Remove specified hashtags from the content column
hashtags_to_remove = ['#blm','#BLACK_LIVES_MATTER', '#black_lives_matter','#BlackLivesMatter' ,'#BlackLivesMatterUK','#BlackLivesMattterUK','#BlackLivesMatttersUK', '#AllBlackLivesMatter','#blacklivesmatter', '#BLACKlivesmatter', '#black_lifes_matter', '#BLM', '#blmProtest', '#BLACKLIVESMATTER','#BlackLivesmatter']
df['content'] = df['content'].apply(lambda x: remove_hashtags(x, hashtags_to_remove))

# Function to extract hashtags from text
def extract_hashtags(text):
    return re.findall(r'#\w+', text)

# Count hashtag occurrences
hashtags_counter = Counter()
for content in df['content']:
    hashtags = extract_hashtags(content)
    hashtags_counter.update(hashtags)

# Get top 300 most frequent hashtags after removing specified hashtags
top_300_hashtags = [hashtag for hashtag, _ in hashtags_counter.most_common(200)]

# Build co-occurrence graph using only top 300 hashtags
co_occurrence_graph = defaultdict(lambda: defaultdict(int))

for content in tqdm(df['content']):
    hashtags = extract_hashtags(content)
    hashtags = [hashtag for hashtag in hashtags if hashtag in top_300_hashtags]
    for i in range(len(hashtags)):
        for j in range(i+1, len(hashtags)):
            co_occurrence_graph[hashtags[i]][hashtags[j]] += 1
            co_occurrence_graph[hashtags[j]][hashtags[i]] += 1

# Remove edges with weights less than 10
co_occurrence_graph_filtered = defaultdict(lambda: defaultdict(int))

for hashtag, neighbors in co_occurrence_graph.items():
    for neighbor, weight in neighbors.items():
        if weight >= 10:
            co_occurrence_graph_filtered[hashtag][neighbor] = weight

# Create a directed graph
G = nx.Graph()

# Add nodes and edges with weights
for hashtag, neighbors in co_occurrence_graph_filtered.items():
    if neighbors:  # Add only if the node has neighbors
        for neighbor, weight in neighbors.items():
            G.add_edge(hashtag, neighbor, weight=weight)

# Find connected components (clusters)
clusters = list(nx.connected_components(G))

# Plot the graph with clusters
plt.figure(figsize=(12, 8))
pos = nx.spring_layout(G, seed=42)  # Set the layout for consistent positioning

# Draw nodes with different colors for each cluster
for i, cluster in enumerate(clusters):
    nx.draw_networkx_nodes(G, pos, nodelist=list(cluster), node_color=plt.cm.tab20(i), node_size=1000, label=f'Cluster {i+1}')

# Draw edges
nx.draw_networkx_edges(G, pos, width=0.5, alpha=0.7, edge_color='gray')

# Draw labels
nx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif')

# Add legend
plt.legend()

plt.title('Co-occurrence Graph with Clusters')
plt.show()

# Print number of nodes after removing nodes with no edges
print("Number of nodes after removing nodes with no edges:", G.number_of_nodes())

# Print all clusters
for i, cluster in enumerate(clusters):
    print(f'Cluster {i+1}: {list(cluster)}')



from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Define a function to generate and plot word clouds for each cluster
def plot_word_clouds(clusters):
    for i, cluster in enumerate(clusters):
        # Concatenate all hashtags in the cluster into a single string
        text = ' '.join(cluster)

        # Generate word cloud
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

        # Plot word cloud
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.title(f'Word Cloud for Cluster {i+1}')
        plt.axis('off')
        plt.show()

# Plot word clouds for all clusters
plot_word_clouds(clusters)

!pip install pyvis

import pandas as pd
import re
import matplotlib.pyplot as plt
import networkx as nx
from collections import defaultdict, Counter
from tqdm import tqdm
from pyvis.network import Network


df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)


def remove_hashtags(text, hashtags_to_remove):
    for hashtag in hashtags_to_remove:
        text = text.replace(hashtag, '')
    return text


hashtags_to_remove = ['#blm','#BLACK_LIVES_MATTER', '#black_lives_matter','#BlackLivesMatter' , '#AllBlackLivesMatter','#blacklivesmatter', '#BLACKlivesmatter', '#black_lifes_matter', '#BLM', '#blmProtest', '#BLACKLIVESMATTER','#BlackLivesmatter']
df['content'] = df['content'].apply(lambda x: remove_hashtags(x, hashtags_to_remove))

def extract_hashtags(text):
    return re.findall(r'#\w+', text)

hashtags_counter = Counter()
for content in df['content']:
    hashtags = extract_hashtags(content)
    hashtags_counter.update(hashtags)


top_300_hashtags = [hashtag for hashtag, _ in hashtags_counter.most_common(200)]

co_occurrence_graph = defaultdict(lambda: defaultdict(int))

for content in tqdm(df['content']):
    hashtags = extract_hashtags(content)
    hashtags = [hashtag for hashtag in hashtags if hashtag in top_300_hashtags]
    for i in range(len(hashtags)):
        for j in range(i+1, len(hashtags)):
            co_occurrence_graph[hashtags[i]][hashtags[j]] += 1
            co_occurrence_graph[hashtags[j]][hashtags[i]] += 1

co_occurrence_graph_filtered = defaultdict(lambda: defaultdict(int))

for hashtag, neighbors in co_occurrence_graph.items():
    for neighbor, weight in neighbors.items():
        if weight >= 10:
            co_occurrence_graph_filtered[hashtag][neighbor] = weight

G = nx.Graph()

for hashtag, neighbors in co_occurrence_graph_filtered.items():
    if neighbors:  # Add only if the node has neighbors
        for neighbor, weight in neighbors.items():
            G.add_edge(hashtag, neighbor, weight=weight)

clusters = list(nx.connected_components(G))

net = Network(height='100%', width='100%', directed=False)
net.from_nx(G)

for i, cluster in enumerate(clusters):
    for node in cluster:
        net.add_node(node, label=f'Cluster {i+1}', color=plt.cm.tab20(i))

net.show_buttons()
net.write_html('pyvis_graph.html')

import pandas as pd
import numpy as np
import re
from collections import Counter

# Load your dataset
df = pd.read_csv('blm_11k.csv', delimiter='\t')

# Hashtags to remove from the corpus
hashtags_to_remove = ['blm','BLACK_LIVES_MATTER', 'black_lives_matter','BlackLivesMatter' ,'BlackLivesMatterUK','BlackLivesMattterUK','BlackLivesMatttersUK', 'AllBlackLivesMatter','blacklivesmatter', 'BLACKlivesmatter', 'black_lifes_matter', 'BLM', 'blmProtest', 'BLACKLIVESMATTER','BlackLivesmatter']

# Extract hashtags from the content column and remove specified hashtags
def extract_hashtags(text):
    hashtags = set(re.findall(r'#(\w+)', text.lower()))  # Convert to lowercase
    return hashtags - set(hashtags_to_remove)

df['hashtags'] = df['content'].apply(lambda x: extract_hashtags(x))

# Count frequency of remaining hashtags
all_hashtags = [hashtag for hashtags in df['hashtags'] for hashtag in hashtags]
top_hashtags = Counter(all_hashtags).most_common(200)

# Extract the top 300 hashtags
selected_hashtags = [hashtag for hashtag, _ in top_hashtags]

# Initialize an empty DataFrame to store the embedding vectors
embedding_df = pd.DataFrame(0, index=np.arange(len(df)), columns=selected_hashtags)

# Populate the embedding DataFrame
for i, hashtags in enumerate(df['hashtags']):
    for hashtag in hashtags:
        if hashtag in selected_hashtags:
            embedding_df.at[i, hashtag] = 1

# Convert the DataFrame to a NumPy array
embedding_matrix = embedding_df.to_numpy()

# Now, each row in the embeQdding_matrix represents the embedding vector for each tweet,
# where each column corresponds to a selected hashtag, and the value is 1 if the hashtag is present
# in the tweet and 0 otherwise.

import numpy as np
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.metrics.pairwise import pairwise_distances
from concurrent.futures import ProcessPoolExecutor
from tqdm import tqdm

# Function to calculate pairwise distances for a chunk
def calculate_chunk_distances(chunk):
    return pairwise_distances(chunk, metric='euclidean')

# Divide the embedding matrix into chunks for parallel processing
chunk_size = 1000  # Adjust as needed
embedding_chunks = [embedding_matrix[i:i+chunk_size] for i in range(0, len(embedding_matrix), chunk_size)]

# Pad the last chunk if its size is less than the desired chunk size
last_chunk_size = len(embedding_matrix) % chunk_size
if last_chunk_size > 0:
    padding = np.zeros((chunk_size - last_chunk_size, embedding_matrix.shape[1]))
    embedding_chunks[-1] = np.concatenate([embedding_chunks[-1], padding], axis=0)

# Perform parallel computation of pairwise distances
with ProcessPoolExecutor() as executor:
    chunk_distances = list(tqdm(executor.map(calculate_chunk_distances, embedding_chunks), total=len(embedding_chunks), desc="Calculating pairwise distances"))

# Concatenate the chunk distances
distances = np.concatenate(chunk_distances)

# Quantize the distances to reduce precision
distances_quantized = np.round(distances, decimals=2).astype(np.float32)

# Perform hierarchical clustering
Z = linkage(distances_quantized, method='ward')

# Extract clusters
num_clusters = 5  # Adjust as needed
clusters = fcluster(Z, t=num_clusters, criterion='maxclust')

# Write clusters to a file
with open("clusters.txt", "w") as file:
    file.write("Clusters:\n")
    for i in range(1, num_clusters + 1):
        file.write(f"Cluster {i}:\n")
        # Find indices of hashtags belonging to cluster i
        indices = [idx for idx, label in enumerate(clusters) if label == i]
        # Write hashtags corresponding to the indices
        for idx in indices:
            hashtag = all_hashtags[idx]  # Access hashtag directly from the list
            file.write(f"{hashtag}\n")

# Print total number of clusters
print(f"Total number of clusters: {num_clusters}")

# Calculate the total number of unique hashtags
total_unique_hashtags = len(set(all_hashtags))

# Print the total number of unique hashtags
print(f"Total number of unique hashtags: {total_unique_hashtags}")

