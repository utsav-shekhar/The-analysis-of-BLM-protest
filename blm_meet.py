# -*- coding: utf-8 -*-
"""blm_meet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wXN2Yc07auQObIaTvzrEPLQ-I-k_5sGV
"""

import pandas as pd
import re

# Read the CSV file into a DataFrame
df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)

# Function to extract hashtags from text
def extract_hashtags(text):
    if isinstance(text, str):
        # Regular expression to extract hashtags
        hashtags = re.findall(r'#(\w+)', text)
        # Filter out hashtags that are also URLs
        hashtags = [tag for tag in hashtags if not tag.startswith('http')]
        return hashtags
    else:
        return []

# Apply the extract_hashtags function to the 'content' column
df['hashtags'] = df['content'].apply(extract_hashtags)

# Flatten the list of hashtags and convert them to lowercase
all_hashtags = [item.lower() for sublist in df['hashtags'].dropna() for item in sublist]

# Count the occurrences of each hashtag
hashtag_counts = pd.Series(all_hashtags).value_counts()

# Print the top 20 most frequent hashtags
print(hashtag_counts.head(20))

import plotly.express as px

tag_frequency_df = pd.DataFrame({'Tag': hashtag_counts.index, 'Frequency': hashtag_counts.values})
tag_frequency_df = tag_frequency_df.sort_values(by='Frequency', ascending=False)

fig = px.bar(tag_frequency_df.head(25), x='Tag', y='Frequency', title='Top 25 Hashtags Frequency',
             labels={'Frequency': 'Frequency', 'Tag': 'Hashtags'}, height=500)

fig.update_layout(
    xaxis=dict(title='Hashtags'),
    yaxis=dict(title='Frequency'),
)


fig.show()

import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)

import re
def extract_hashtags(text):
    if isinstance(text, str):
        # Regular expression to extract hashtags
        hashtags = re.findall(r'#(\w+)', text)
        # Filter out hashtags that are also URLs
        hashtags = [tag for tag in hashtags if not tag.startswith('http')]
        return hashtags
    else:
        return []

import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from sklearn.feature_extraction.text import CountVectorizer

df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)  # Limit to the first 11000 rows

def extract_hashtags(text):
    if isinstance(text, str):
        # Regular expression to extract hashtags
        hashtags = re.findall(r'#(\w+)', text)
        # Filter out hashtags that are also URLs
        hashtags = [tag for tag in hashtags if not tag.startswith('http')]
        return hashtags
    else:
        return []

df['hashtags'] = df['content'].apply(extract_hashtags)

# Replace NaN values with empty strings
df['content'] = df['content'].fillna('')

all_hashtags = [item.lower() for sublist in df['hashtags'].dropna() for item in sublist]  # Convert to lowercase

hashtag_counts = pd.Series(all_hashtags).value_counts()

top_100_hashtags = hashtag_counts.head(20).index
vectorizer = CountVectorizer(binary=True, vocabulary=top_100_hashtags)
hashtag_matrix = pd.DataFrame(vectorizer.fit_transform(df['content']).toarray(), columns=vectorizer.get_feature_names_out())

correlation_matrix = hashtag_matrix.T.dot(hashtag_matrix)

cmap = sns.color_palette("Blues", as_cmap=True)
vmin, vmax = 0, 100
fig = go.Figure()

fig.add_trace(go.Heatmap(
    x=correlation_matrix.columns,
    y=correlation_matrix.index,
    z=correlation_matrix.values,
    colorscale='Blues',
    zmin=vmin,
    zmax=vmax,
    colorbar=dict(tickvals=list(range(0, 101, 10)), ticktext=list(map(str, range(0, 101, 10)))),
))

fig.update_layout(
    title='Correlation Matrix Heatmap (Top 100 Hashtags)',
    xaxis=dict(ticks='', showgrid=False, zeroline=False),
    yaxis=dict(ticks='', showgrid=False, zeroline=False),
)

fig.show()

import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer

# Define the list of hashtags to remove (without the '#')
hashtags_to_remove = ['blm', 'BLACK_LIVES_MATTER', 'black_lives_matter', 'BlackLivesMatter', 'BlackLivesMatterUK', 'BlackLivesMattterUK', 'BlackLivesMatttersUK', 'AllBlackLivesMatter', 'blacklivesmatter', 'BLACKlivesmatter', 'black_lifes_matter', 'BLM', 'blmProtest', 'BLACKLIVESMATTER', 'BlackLivesmatter']

# Read the CSV file into a DataFrame
df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)  # Limit to the first 11000 rows

# Function to remove specified hashtags from text
def remove_hashtags(text):
    if isinstance(text, str):
        # Remove specified hashtags
        for hashtag in hashtags_to_remove:
            text = text.replace('#' + hashtag, '')
        return text
    else:
        return ''

# Remove specified hashtags from the content
df['content'] = df['content'].apply(remove_hashtags)

# Convert hashtags to lowercase and flatten the list
all_hashtags = [item.lower() for sublist in df['content'].str.findall(r'#(\w+)').dropna() for item in sublist]

# Count the occurrences of each hashtag
hashtag_counts = pd.Series(all_hashtags).value_counts()

# Get the top 100 hashtags after removing specified hashtags
top_100_hashtags = hashtag_counts.head(100).index

# Create CountVectorizer with top 100 hashtags
vectorizer = CountVectorizer(binary=True, vocabulary=top_100_hashtags)

# Fill NaN values in the 'content' column with empty strings
df['content'] = df['content'].fillna('')

# Transform 'content' column into a matrix of token counts
hashtag_matrix = pd.DataFrame(vectorizer.fit_transform(df['content']).toarray(), columns=vectorizer.get_feature_names_out())

# Compute correlation matrix
correlation_matrix = hashtag_matrix.T.dot(hashtag_matrix)

# Sort the correlation matrix by the sum of each row (total counts for each hashtag)
sorted_correlation_matrix = correlation_matrix.loc[correlation_matrix.sum(axis=1).sort_values(ascending=False).index]

# Create heatmap with Matplotlib
plt.figure(figsize=(15, 10))
sns.heatmap(sorted_correlation_matrix, cmap="Blues", vmin=0, vmax=100)
plt.title('Correlation Matrix Heatmap (Top 100 Hashtags)')
plt.xlabel('Hashtags')
plt.ylabel('Hashtags')
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.show()

import pandas as pd
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans
df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)
def extract_hashtags(text):
    return re.findall(r'#(\w+)', text)

df['hashtags'] = df['content'].apply(extract_hashtags)
all_hashtags = list(set([item.lower() for sublist in df['hashtags'].dropna() for item in sublist]))
vectorizer = CountVectorizer(binary=True, vocabulary=all_hashtags)
hashtag_matrix = pd.DataFrame(vectorizer.fit_transform(df['content']).toarray(), columns=vectorizer.get_feature_names_out())

relevant_tags = ['blm', 'blacklivesmatter']
cooccurrence_dict = {}

for tag in all_hashtags:
    cooccurrence_sum = sum(df['content'].str.contains(f'#{tag}.*#{relevant_tags[0]}|#{relevant_tags[0]}.*#{tag}|#{tag}.*#{relevant_tags[1]}|#{relevant_tags[1]}.*#{tag}', case=False))
    cooccurrence_dict[tag] = cooccurrence_sum
print("Co-occurrence Dictionary:")
print(cooccurrence_dict)

import pandas as pd
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)
def extract_hashtags(text):
    return re.findall(r'#(\w+)', text)

df['hashtags'] = df['content'].apply(extract_hashtags)
all_hashtags = list(set([item.lower() for sublist in df['hashtags'].dropna() for item in sublist]))
vectorizer = CountVectorizer(binary=True, vocabulary=all_hashtags)
hashtag_matrix = pd.DataFrame(vectorizer.fit_transform(df['content']).toarray(), columns=vectorizer.get_feature_names_out())
relevant_tags = ['blm', 'blacklivesmatter']
cooccurrence_dict = {}

for tag in all_hashtags:
    cooccurrence_sum = sum(df['content'].str.contains(f'#{tag}.*#{relevant_tags[0]}|#{relevant_tags[0]}.*#{tag}|#{tag}.*#{relevant_tags[1]}|#{relevant_tags[1]}.*#{tag}', case=False))
    cooccurrence_dict[tag] = cooccurrence_sum
cooccurrence_df = pd.DataFrame(list(cooccurrence_dict.items()), columns=['Tag', 'Cooccurrence_Sum'])
cooccurrence_df.set_index('Tag', inplace=True)

inertia_values = []
possible_k_values = range(1, 11)

for k in possible_k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(cooccurrence_df[['Cooccurrence_Sum']])
    inertia_values.append(kmeans.inertia_)

plt.plot(possible_k_values, inertia_values, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia (Within-Cluster Sum of Squares)')
plt.show()

import pandas as pd
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from wordcloud import WordCloud

df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)

def extract_hashtags(text):
    return re.findall(r'#(\w+)', text)

df['hashtags'] = df['content'].apply(extract_hashtags)

# Remove "blm" and "blacklivesmatter" from the list of hashtags
all_hashtags = list(set([item.lower() for sublist in df['hashtags'].dropna() for item in sublist if item.lower() not in ['blm', 'blacklivesmatter']]))

vectorizer = CountVectorizer(binary=True, vocabulary=all_hashtags)
hashtag_matrix = pd.DataFrame(vectorizer.fit_transform(df['content']).toarray(), columns=vectorizer.get_feature_names_out())

cooccurrence_dict = {}

for tag in all_hashtags:
    cooccurrence_sum = sum(df['content'].str.contains(f'#{tag}', case=False))
    cooccurrence_dict[tag] = cooccurrence_sum

cooccurrence_df = pd.DataFrame(list(cooccurrence_dict.items()), columns=['Tag', 'Cooccurrence_Sum'])
cooccurrence_df.set_index('Tag', inplace=True)

optimal_k = 6
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
cooccurrence_df['Cluster'] = kmeans.fit_predict(cooccurrence_df[['Cooccurrence_Sum']])

# Define a function to create word clouds for each cluster
def generate_wordcloud(text):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()

# Iterate over each cluster and generate a word cloud
for cluster_id in range(optimal_k):
    cluster_tags = cooccurrence_df[cooccurrence_df['Cluster'] == cluster_id].index
    wordcloud_text = ' '.join(cluster_tags)

    print(f"Cluster {cluster_id + 1} Tags:")
    print(", ".join(cluster_tags))

    generate_wordcloud(wordcloud_text)

optimal_k = 6
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
cooccurrence_df['Cluster'] = kmeans.fit_predict(cooccurrence_df[['Cooccurrence_Sum']])
for cluster_id in range(optimal_k):
    cluster_tags = cooccurrence_df[cooccurrence_df['Cluster'] == cluster_id].index
    avg_cooccurrence = cooccurrence_df[cooccurrence_df['Cluster'] == cluster_id]['Cooccurrence_Sum'].mean()

    print(f"Cluster {cluster_id + 1} Tags:")
    print(", ".join(cluster_tags))
    print(f"Avg Co-occurrence for Cluster {cluster_id + 1}: {avg_cooccurrence}\n")

import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 6))
sns.boxplot(x='Cluster', y='Cooccurrence_Sum', data=cooccurrence_df)
plt.title('Box Plot of Co-occurrence Sums within Clusters')
plt.show()

plt.figure(figsize=(12, 6))
sns.violinplot(x='Cluster', y='Cooccurrence_Sum', data=cooccurrence_df)
plt.title('Violin Plot of Co-occurrence Sums within Clusters')
plt.show()

for cluster_id in range(1, optimal_k):
    cluster_tags = cooccurrence_df[cooccurrence_df['Cluster'] == cluster_id].index
    avg_cooccurrence = cooccurrence_df[cooccurrence_df['Cluster'] == cluster_id]['Cooccurrence_Sum'].mean()

    print(f"Cluster {cluster_id + 1} Tags:")
    print(", ".join(cluster_tags))
    print(f"Avg Co-occurrence for Cluster {cluster_id + 1}: {avg_cooccurrence}\n")

import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from sklearn.feature_extraction.text import CountVectorizer

df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)  # Limit to the first 11000 rows

def extract_hashtags(text):
    return re.findall(r'#(\w+)', text)

df['hashtags'] = df['content'].apply(extract_hashtags)

# Remove "blm" and "blacklivesmatter" from the list of hashtags
hashtags_to_remove = ['#blm','#BLACK_LIVES_MATTER', '#black_lives_matter','#BlackLivesMatter' ,'#BlackLivesMatterUK','#BlackLivesMattterUK','#BlackLivesMatttersUK', '#AllBlackLivesMatter','#blacklivesmatter', '#BLACKlivesmatter', '#black_lifes_matter', '#BLM', '#blmProtest', '#BLACKLIVESMATTER','#BlackLivesmatter']
all_hashtags = [item.lower() for sublist in df['hashtags'].dropna() for item in sublist if item.lower() not in hashtags_to_remove ]

hashtag_counts = pd.Series(all_hashtags).value_counts()

top_100_hashtags = hashtag_counts.head(20).index
vectorizer = CountVectorizer(binary=True, vocabulary=top_100_hashtags)
hashtag_matrix = pd.DataFrame(vectorizer.fit_transform(df['content']).toarray(), columns=vectorizer.get_feature_names_out())

correlation_matrix = hashtag_matrix.T.dot(hashtag_matrix)
custom_color_scale = 'Jet'  # Rainbow color scale
max_value = min(1000, correlation_matrix.values.max())  # Set the maximum value to 1000 or the maximum value in the matrix, whichever is smaller

fig = go.Figure()

fig.add_trace(go.Heatmap(
    x=correlation_matrix.columns,
    y=correlation_matrix.index,
    z=correlation_matrix.values,
    colorscale=custom_color_scale,
    zmin=0,
    zmax=max_value,
    colorbar=dict(tickvals=list(range(0, max_value+1, max_value//7)), ticktext=list(map(str, range(0, max_value+1, max_value//7)))),
))

fig.update_layout(
    title='Correlation Matrix Heatmap (Top 100 Hashtags)',
    xaxis=dict(ticks='', showgrid=False, zeroline=False),
    yaxis=dict(ticks='', showgrid=False, zeroline=False),
)

fig.show()

import pandas as pd
import re
import numpy as np
import plotly.express as px
from sklearn.feature_extraction.text import CountVectorizer

# Load your dataset
df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=1000)

# Extract hashtags from the content column
def extract_hashtags(text):
    return ' '.join(re.findall(r'#(\w+)', text.lower()))  # Convert to lowercase

df['hashtags'] = df['content'].apply(extract_hashtags)

# Create a Term Document Matrix using CountVectorizer
vectorizer = CountVectorizer(binary=True)
tdm = vectorizer.fit_transform(df['hashtags']).toarray()

# Calculate normalized ranks based on the formula
tdm_normalized = np.zeros_like(tdm, dtype=float)

for i in range(tdm.shape[0]):  # Loop through tweets
    for j in range(tdm.shape[1]):  # Loop through hashtags
        if tdm[i, j] > 0:
            rk = np.argsort(tdm[i])[::-1].tolist().index(j) + 1  # Get the rank of the hashtag in the tweet
            k = np.sum(tdm[i] > 0)  # Total length of the list (non-zero elements)
            tdm_normalized[i, j] = (rk / k) ** -1  # Apply the formula

# Convert the normalized matrix to a DataFrame
tdm_normalized_df = pd.DataFrame(tdm_normalized, columns=vectorizer.get_feature_names_out())

# Plot an interactive heatmap using Plotly
fig = px.imshow(tdm_normalized_df, labels=dict(x="Hashtags", y="Tweets", color="Normalized Rank"),
                x=tdm_normalized_df.columns, y=tdm_normalized_df.index,
                color_continuous_scale="Rainbow", range_color=(0, 1))

# Customize layout
fig.update_layout(title='Interactive Heatmap of Normalized Ranks in Term Document Matrix',
                  xaxis_title='Hashtags', yaxis_title='Tweets',
                  coloraxis_colorbar=dict(tickvals=np.linspace(0, 1, 8), ticktext=np.linspace(0, 7000, 8).astype(int)))

# Show the interactive plot
fig.show()

import pandas as pd
import re
import numpy as np
import plotly.graph_objects as go
from sklearn.feature_extraction.text import CountVectorizer

# Load your dataset
df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=2000)

# Extract hashtags from the content column
def extract_hashtags(text):
    return re.findall(r'#(\w+)', text.lower())  # Convert to lowercase

df['hashtags'] = df['content'].apply(extract_hashtags)

# Create a list of unique tags
all_tags = list(set([tag for sublist in df['hashtags'] for tag in sublist]))

# Create a DataFrame with each row representing a tweet and each column representing a unique tag
tag_matrix = np.zeros((len(df), len(all_tags)), dtype=int)
for i, tags in enumerate(df['hashtags']):
    for tag in tags:
        if tag in all_tags:
            j = all_tags.index(tag)
            tag_matrix[i, j] = 1

# Convert the tag matrix to a DataFrame
tag_df = pd.DataFrame(tag_matrix, columns=all_tags)

# Plot the interactive heatmap
fig = go.Figure(data=go.Heatmap(
    z=tag_df.values,
    x=tag_df.columns,
    y=np.arange(1, len(df) + 1),
    colorscale='Rainbow',
    colorbar=dict(title='Tag Occurrence'),
))

fig.update_layout(
    title='Interactive Term-Document Matrix (TDM) Heatmap',
    xaxis=dict(title='Unique Tags'),
    yaxis=dict(title='Tweet Numbers'),
)

# Show the plot
fig.show()

import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt

df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)

def extract_hashtags(text):
    return re.findall(r'#(\w+)', text)

df['hashtags'] = df['content'].apply(extract_hashtags)

all_hashtags = [tag.lower() for sublist in df['hashtags'].dropna() for tag in sublist]

tag_df = pd.DataFrame(all_hashtags, columns=['tag'])
tag_occurrences = tag_df['tag'].value_counts()

wordcloud = WordCloud(width=800, height=400, background_color='white')

wordcloud.generate_from_frequencies(tag_occurrences)

# Plot WordCloud
plt.figure(figsize=(10, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # Remove axis
plt.title('Term-Document Matrix (TDM) - Word Cloud of Hashtags')
plt.show()

import pandas as pd
import plotly.graph_objects as go
from sklearn.feature_extraction.text import CountVectorizer

# Load your dataset
df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=3000)

# Extract hashtags from the content column
def extract_hashtags(text):
    return ' '.join(re.findall(r'#(\w+)', text.lower()))  # Convert to lowercase

df['hashtags'] = df['content'].apply(extract_hashtags)

# Create a Term Document Matrix using CountVectorizer
vectorizer = CountVectorizer(binary=True)
tdm = vectorizer.fit_transform(df['hashtags']).toarray()
tdm_df = pd.DataFrame(tdm, columns=vectorizer.get_feature_names_out())

# Create a heatmap using Plotly
fig = go.Figure(data=go.Heatmap(
                   z=tdm_df.values,
                   x=tdm_df.columns,
                   y=tdm_df.index,
                   colorscale='viridis'))

# Customize the layout
fig.update_layout(title='Term Document Matrix Heatmap',
                  xaxis_title='Hashtags',
                  yaxis_title='Tweets')

# Show the interactive plot
fig.show()

import pandas as pd
import re
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances

df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)

def extract_hashtags(text):
    return re.findall(r'#(\w+)', text)

df['hashtags'] = df['content'].apply(extract_hashtags)

# Remove "blm" and "blacklivesmatter" from the list of hashtags
all_hashtags = list(set([item.lower() for sublist in df['hashtags'].dropna() for item in sublist if item.lower() not in ['blm', 'blacklivesmatter']]))

# Create a co-occurrence matrix
co_occurrence_matrix = np.zeros((len(all_hashtags), len(all_hashtags)))

for tags in df['hashtags']:
    for i, tag1 in enumerate(all_hashtags):
        if tag1 in tags:
            for j, tag2 in enumerate(all_hashtags):
                if tag2 in tags and tag1 != tag2:
                    co_occurrence_matrix[i][j] += 1

# Normalize the co-occurrence matrix
co_occurrence_matrix /= np.max(co_occurrence_matrix)

# Cluster the hashtags based on the co-occurrence matrix
optimal_k = 6
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
cluster_labels = kmeans.fit_predict(pairwise_distances(co_occurrence_matrix, metric='euclidean'))

# Assign each hashtag to its cluster
hashtags_clusters = {tag: cluster for tag, cluster in zip(all_hashtags, cluster_labels)}

# Print the hashtags in each cluster
for cluster_id in range(optimal_k):
    cluster_tags = [tag for tag, cluster in hashtags_clusters.items() if cluster == cluster_id]
    print(f"Cluster {cluster_id + 1} Tags:")
    print(", ".join(cluster_tags))

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Define a function to generate word clouds
def generate_wordcloud(text):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()

# Generate word clouds for each cluster and print the number of tags in each cluster
for cluster_id in range(optimal_k):
    cluster_tags = [tag for tag, cluster in hashtags_clusters.items() if cluster == cluster_id]
    wordcloud_text = ' '.join(cluster_tags)

    print(f"Cluster {cluster_id + 1} Tags ({len(cluster_tags)} tags):")
    generate_wordcloud(wordcloud_text)

import pandas as pd
import re

df = pd.read_csv('blm_11k.csv', delimiter='\t', nrows=11000)

def extract_hashtags(text):
    return re.findall(r'#(\w+)', text)

df['hashtags'] = df['content'].apply(extract_hashtags)

all_hashtags = [item.lower() for sublist in df['hashtags'].dropna() for item in sublist]

hashtag_counts = pd.Series(all_hashtags).value_counts()

top_300_hashtags = hashtag_counts.head(300).index.tolist()
# Remove specified hashtags
hashtags_to_remove = ['blm', 'blacklivesmatter','BLACK_LIVES_MATTER', 'black_lives_matter', 'BlackLivesMatter', 'AllBlackLivesMatter', 'blacklivesmatter', 'BLACKlivesmatter', 'black_lifes_matter', 'BLM', 'blmProtest', 'BLACKLIVESMATTER', 'BlackLivesmatter']
filtered_hashtags = [tag for tag in top_300_hashtags if tag not in hashtags_to_remove]

def co_occurrence_matrix(hashtags, data):
    co_occur = [[0]*len(hashtags) for _ in range(len(hashtags))]
    for tags in data['hashtags']:
        for i, tag1 in enumerate(hashtags):
            if tag1 in tags:
                for j, tag2 in enumerate(hashtags):
                    if tag2 in tags and tag1 != tag2:
                        co_occur[i][j] += 1
    return co_occur

co_occurrence = co_occurrence_matrix(filtered_hashtags, df)

# Print first 10x10 submatrix as an example
for row in co_occurrence[:10]:
    print(row[:10])

co_occurrence

import numpy as np

# Assuming coocc_matrix is your co-occurrence matrix
U, s, Vh = np.linalg.svd(co_occurrence)
k = 100
U_k = U[:, :k]
s_k = s[:k]
Vh_k = Vh[:k, :]

# Compute word embeddings
word_embeddings = U_k @ np.diag(np.sqrt(s_k))

# Assuming top_300_hashtags contains the top 300 hashtags by frequency
for i in range(10):
    print(top_300_hashtags[i], word_embeddings[i])

import matplotlib.pyplot as plt

# Define a range of cluster numbers to try
k_values = range(1, 20)  # You can adjust the range as needed

# Calculate the sum of squared distances for different values of k
wcss = []
for k in k_values:
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(word_embeddings)
    wcss.append(kmeans.inertia_)

# Plot the elbow curve
plt.figure(figsize=(8, 6))
plt.plot(k_values, wcss, marker='o', linestyle='-', color='b')
plt.title('Elbow Curve')
plt.xlabel('Number of Clusters')
plt.ylabel('Within-Cluster Sum of Squares (WCSS)')
plt.xticks(k_values)
plt.grid(True)
plt.show()

# Using KMeans with 9 clusters
kmeans = KMeans(n_clusters=9)
kmeans.fit(word_embeddings)

# Get cluster labels
cluster_labels = kmeans.labels_

# Initialize clusters as dictionaries
clusters = {i: [] for i in range(9)}

# Assign each hashtag to its cluster
for i, label in enumerate(cluster_labels):
    clusters[label].append(top_300_hashtags[i])

# Print the clusters
for cluster_num, hashtags in clusters.items():
    print(f"Cluster {cluster_num}:")
    print(", ".join(hashtags))
    print()

import pandas as pd
import numpy as np
import re
from collections import Counter

# Load your dataset
df = pd.read_csv('blm_11k.csv', delimiter='\t')

# Hashtags to remove from the corpus
hashtags_to_remove = ['blm','BLACK_LIVES_MATTER', 'black_lives_matter','BlackLivesMatter' ,'BlackLivesMatterUK','BlackLivesMattterUK','BlackLivesMatttersUK', 'AllBlackLivesMatter','blacklivesmatter', 'BLACKlivesmatter', 'black_lifes_matter', 'BLM', 'blmProtest', 'BLACKLIVESMATTER','BlackLivesmatter']

# Extract hashtags from the content column and remove specified hashtags
def extract_hashtags(text):
    hashtags = set(re.findall(r'#(\w+)', text.lower()))  # Convert to lowercase
    return hashtags - set(hashtags_to_remove)

df['hashtags'] = df['content'].apply(lambda x: extract_hashtags(x))

# Count frequency of remaining hashtags
all_hashtags = [hashtag for hashtags in df['hashtags'] for hashtag in hashtags]
top_hashtags = Counter(all_hashtags).most_common(200)

# Extract the top 300 hashtags
selected_hashtags = [hashtag for hashtag, _ in top_hashtags]

# Initialize an empty DataFrame to store the embedding vectors
embedding_df = pd.DataFrame(0, index=np.arange(len(df)), columns=selected_hashtags)

# Populate the embedding DataFrame
for i, hashtags in enumerate(df['hashtags']):
    for hashtag in hashtags:
        if hashtag in selected_hashtags:
            embedding_df.at[i, hashtag] = 1

# Convert the DataFrame to a NumPy array
embedding_matrix = embedding_df.to_numpy()

# Now, each row in the embeQdding_matrix represents the embedding vector for each tweet,
# where each column corresponds to a selected hashtag, and the value is 1 if the hashtag is present
# in the tweet and 0 otherwise.

embedding_matrix[1:10]

print("Shape of the embedding matrix:", embedding_matrix.shape)

len(embedding_matrix)

import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.metrics.pairwise import pairwise_distances
from concurrent.futures import ProcessPoolExecutor
from tqdm import tqdm

# Function to calculate pairwise distances for a chunk
def calculate_chunk_distances(chunk):
    return pairwise_distances(chunk, metric='euclidean')

# Define the number of hashtags to consider
num_hashtags = 300

# Divide the embedding matrix into chunks for parallel processing
chunk_size = 1000  # Adjust as needed
embedding_chunks = [embedding_matrix[i:i+chunk_size] for i in range(0, len(embedding_matrix), chunk_size)]

# Pad the last chunk if its size is less than the desired chunk size
last_chunk_size = len(embedding_matrix) % chunk_size
if last_chunk_size > 0:
    padding = np.zeros((chunk_size - last_chunk_size, embedding_matrix.shape[1]))
    embedding_chunks[-1] = np.concatenate([embedding_chunks[-1], padding], axis=0)

# Perform parallel computation of pairwise distances
with ProcessPoolExecutor() as executor:
    chunk_distances = list(tqdm(executor.map(calculate_chunk_distances, embedding_chunks), total=len(embedding_chunks), desc="Calculating pairwise distances"))

# Concatenate the chunk distances
distances = np.concatenate(chunk_distances)

# Quantize the distances to reduce precision
distances_quantized = np.round(distances, decimals=2).astype(np.float32)

# Perform hierarchical clustering
Z = linkage(distances_quantized, method='ward')

# Plot dendrogram
plt.figure(figsize=(15, 10))
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Hashtags')
plt.ylabel('Distance')
dendrogram(
    Z,
    leaf_rotation=90.,  # rotates the x axis labels
    leaf_font_size=8.,  # font size for the x axis labels
)
plt.show()

import numpy as np
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.metrics.pairwise import pairwise_distances
from concurrent.futures import ProcessPoolExecutor
from tqdm import tqdm

# Function to calculate pairwise distances for a chunk
def calculate_chunk_distances(chunk):
    return pairwise_distances(chunk, metric='euclidean')

# Divide the embedding matrix into chunks for parallel processing
chunk_size = 1000  # Adjust as needed
embedding_chunks = [embedding_matrix[i:i+chunk_size] for i in range(0, len(embedding_matrix), chunk_size)]

# Pad the last chunk if its size is less than the desired chunk size
last_chunk_size = len(embedding_matrix) % chunk_size
if last_chunk_size > 0:
    padding = np.zeros((chunk_size - last_chunk_size, embedding_matrix.shape[1]))
    embedding_chunks[-1] = np.concatenate([embedding_chunks[-1], padding], axis=0)

# Perform parallel computation of pairwise distances
with ProcessPoolExecutor() as executor:
    chunk_distances = list(tqdm(executor.map(calculate_chunk_distances, embedding_chunks), total=len(embedding_chunks), desc="Calculating pairwise distances"))

# Concatenate the chunk distances
distances = np.concatenate(chunk_distances)

# Quantize the distances to reduce precision
distances_quantized = np.round(distances, decimals=2).astype(np.float32)

# Perform hierarchical clustering
Z = linkage(distances_quantized, method='ward')

# Extract clusters
num_clusters = 5  # Adjust as needed
clusters = fcluster(Z, t=num_clusters, criterion='maxclust')

# Write clusters to a file
with open("clusters.txt", "w") as file:
    file.write("Clusters:\n")
    for i in range(1, num_clusters + 1):
        file.write(f"Cluster {i}:\n")
        # Find indices of hashtags belonging to cluster i
        indices = [idx for idx, label in enumerate(clusters) if label == i]
        # Write hashtags corresponding to the indices
        for idx in indices:
            if idx < len(selected_hashtags):  # Ensure the index is within the range of selected_hashtags
                hashtag = selected_hashtags[idx]
                file.write(f"{hashtag}\n")

# Write unique hashtags to a file
unique_hashtags = set(selected_hashtags)
with open("unique_hashtags.txt", "w") as file:
    file.write("Unique Hashtags:\n")
    for hashtag in unique_hashtags:
        file.write(f"{hashtag}\n")

# Print total number of clusters
print(f"Total number of clusters: {num_clusters}")

# Calculate the total number of unique hashtags
total_unique_hashtags = len(unique_hashtags)

# Print the total number of unique hashtags
print(f"Total number of unique hashtags: {total_unique_hashtags}")

import csv

# Define the output CSV file path
output_csv = "embeddings.csv"

# Open the file in write mode
with open(output_csv, "w", newline='') as f:
    # Create a CSV writer object
    writer = csv.writer(f)

    # Write each tag and its embedding to the CSV file
    for tag, embedding in zip(embedding_df.columns, embedding_matrix):
        row = [tag] + list(embedding)
        writer.writerow(row)

print("Shape of the embedding matrix:", embedding_matrix.shape)

